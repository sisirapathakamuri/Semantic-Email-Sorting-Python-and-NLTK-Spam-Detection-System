{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import email\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.metrics import precision, recall, f_measure, ConfusionMatrix\n",
    "from sklearn.model_selection import KFold\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "# Step 1: Data Processing and Tokenization\n",
    "\n",
    "def read_emails(folder_path):\n",
    "    emails = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        with open(os.path.join(folder_path, filename), \"r\", encoding=\"latin1\") as file:\n",
    "            emails.append(file.read())\n",
    "    return emails\n",
    "\n",
    "ham_folder = \"/Users/pavan/Downloads/FinalProjectData/EmailSpamCorpora/corpus/ham\"\n",
    "spam_folder = \"/Users/pavan/Downloads/FinalProjectData/EmailSpamCorpora/corpus/spam\"\n",
    "\n",
    "ham_emails = read_emails(ham_folder)\n",
    "spam_emails = read_emails(spam_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Feature Extraction\n",
    "def extract_features(emails):\n",
    "    features = []\n",
    "    for email_text in emails:\n",
    "        tokens = word_tokenize(email_text)\n",
    "        filtered_tokens = [token.lower() for token in tokens if token.isalpha() and token.lower() not in stopwords.words('english')]\n",
    "        features.extend(filtered_tokens)\n",
    "    return features\n",
    "\n",
    "# Extract features from ham and spam emails\n",
    "ham_features = extract_features(ham_emails)\n",
    "spam_features = extract_features(spam_emails)\n",
    "\n",
    "# Select top N most frequent words as unigram features\n",
    "N = 1000\n",
    "all_features = FreqDist(ham_features + spam_features)\n",
    "word_features = [feature for feature, _ in all_features.most_common(N)]\n",
    "\n",
    "def email_features(email_text):\n",
    "    email_words = set(word_tokenize(email_text))\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features[word] = (word in email_words)\n",
    "    return features\n",
    "\n",
    "# Extract bigram features using collocations\n",
    "bigram_measures = BigramAssocMeasures()\n",
    "ham_bigram_finder = BigramCollocationFinder.from_words(ham_features)\n",
    "ham_bigram_scores = ham_bigram_finder.score_ngrams(bigram_measures.raw_freq)\n",
    "spam_bigram_finder = BigramCollocationFinder.from_words(spam_features)\n",
    "spam_bigram_scores = spam_bigram_finder.score_ngrams(bigram_measures.raw_freq)\n",
    "bigram_features = [bigram for bigram, _ in (ham_bigram_scores + spam_bigram_scores)[:N]]\n",
    "\n",
    "def email_features_with_bigrams(email_text):\n",
    "    email_words = set(word_tokenize(email_text))\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features[word] = (word in email_words)\n",
    "    for bigram in bigram_features:\n",
    "        features[bigram] = (bigram in email_words)\n",
    "    return features\n",
    "\n",
    "# Define new feature function using POS tag counts\n",
    "def pos_features(email_text):\n",
    "    tokens = word_tokenize(email_text)\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    features = {}\n",
    "    for _, tag in tagged_tokens:\n",
    "        features[tag] = features.get(tag, 0) + 1\n",
    "    return features\n",
    "\n",
    "# Convert emails into feature sets\n",
    "ham_feature_sets = [(email_features(email), 'ham') for email in ham_emails]\n",
    "spam_feature_sets = [(email_features(email), 'spam') for email in spam_emails]\n",
    "unigram_feature_sets = [(email_features(email), 'ham') for email in ham_emails] + [(email_features(email), 'spam') for email in spam_emails]\n",
    "bigram_feature_sets = [(email_features_with_bigrams(email), 'ham') for email in ham_emails] + [(email_features_with_bigrams(email), 'spam') for email in spam_emails]\n",
    "pos_feature_sets = [(pos_features(email), 'ham') for email in ham_emails] + [(pos_features(email), 'spam') for email in spam_emails]\n",
    "all_feature_sets = [(email_features(email), 'ham') for email in ham_emails] + [(email_features(email), 'spam') for email in spam_emails]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Naive Bayes Classifier with unigram features:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy (Unigram): 0.9346587054635158\n",
      "\n",
      "Average metrics for 'ham':\n",
      "Precision: 0.7981167608286253\n",
      "Recall: 0.7308951181368479\n",
      "F1-score: 0.7629977183081735\n",
      "\n",
      "Average metrics for 'spam':\n",
      "Precision: 0.38330019880715704\n",
      "Recall: 0.3959198412764297\n",
      "F1-score: 0.38932746030743126\n",
      "\n",
      "Average metrics for 'macro avg':\n",
      "Precision: 0.5907084798178912\n",
      "Recall: 0.5634074797066388\n",
      "F1-score: 0.5761625893078024\n",
      "\n",
      "Average metrics for 'weighted avg':\n",
      "Precision: 0.9914392773644046\n",
      "Recall: 0.9346587054635158\n",
      "F1-score: 0.9615087526943693\n",
      "\n",
      "Results for Naive Bayes Classifier with bigram features:\n",
      "Average Accuracy (Bigram): 0.9346587054635158\n",
      "\n",
      "Average metrics for 'ham':\n",
      "Precision: 0.7981167608286253\n",
      "Recall: 0.7308951181368479\n",
      "F1-score: 0.7629977183081735\n",
      "\n",
      "Average metrics for 'spam':\n",
      "Precision: 0.38330019880715704\n",
      "Recall: 0.3959198412764297\n",
      "F1-score: 0.38932746030743126\n",
      "\n",
      "Average metrics for 'macro avg':\n",
      "Precision: 0.5907084798178912\n",
      "Recall: 0.5634074797066388\n",
      "F1-score: 0.5761625893078024\n",
      "\n",
      "Average metrics for 'weighted avg':\n",
      "Precision: 0.9914392773644046\n",
      "Recall: 0.9346587054635158\n",
      "F1-score: 0.9615087526943693\n",
      "\n",
      "Results for Naive Bayes Classifier with POS features:\n",
      "Average Accuracy (POS): 0.745887739560265\n",
      "\n",
      "Average metrics for 'ham':\n",
      "Precision: 0.7427777777777778\n",
      "Recall: 0.7088852604690835\n",
      "F1-score: 0.7212652563332307\n",
      "\n",
      "Average metrics for 'spam':\n",
      "Precision: 0.3656050955414013\n",
      "Recall: 0.17986651281327565\n",
      "F1-score: 0.2351357846671473\n",
      "\n",
      "Average metrics for 'macro avg':\n",
      "Precision: 0.5541914366595895\n",
      "Recall: 0.44437588664117955\n",
      "F1-score: 0.4782005205001891\n",
      "\n",
      "Average metrics for 'weighted avg':\n",
      "Precision: 0.9530655244681536\n",
      "Recall: 0.745887739560265\n",
      "F1-score: 0.8112176089529312\n",
      "\n",
      "Results for Naive Bayes Classifier with all features:\n",
      "Average Accuracy (All Features): 0.9346587054635158\n",
      "\n",
      "Average metrics for 'ham':\n",
      "Precision: 0.7981167608286253\n",
      "Recall: 0.7308951181368479\n",
      "F1-score: 0.7629977183081735\n",
      "\n",
      "Average metrics for 'spam':\n",
      "Precision: 0.38330019880715704\n",
      "Recall: 0.3959198412764297\n",
      "F1-score: 0.38932746030743126\n",
      "\n",
      "Average metrics for 'macro avg':\n",
      "Precision: 0.5907084798178912\n",
      "Recall: 0.5634074797066388\n",
      "F1-score: 0.5761625893078024\n",
      "\n",
      "Average metrics for 'weighted avg':\n",
      "Precision: 0.9914392773644046\n",
      "Recall: 0.9346587054635158\n",
      "F1-score: 0.9615087526943693\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from nltk.metrics import ConfusionMatrix\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "k = 5  # Number of folds for cross-validation\n",
    "kf = KFold(n_splits=k)\n",
    "\n",
    "# Function to train and evaluate the classifier\n",
    "def train_and_evaluate(classifier, train_set, test_set):\n",
    "    classifier = classifier.train(train_set)\n",
    "\n",
    "    # Evaluate the classifier on the test set\n",
    "    true_labels = [label for _, label in test_set]\n",
    "    predicted_labels = [classifier.classify(features) for features, _ in test_set]\n",
    "\n",
    "    # Calculate evaluation measures\n",
    "    accuracy = nltk.classify.accuracy(classifier, test_set)\n",
    "    confusion_matrix = ConfusionMatrix(true_labels, predicted_labels)\n",
    "    report = classification_report(true_labels, predicted_labels, output_dict=True)\n",
    "\n",
    "    return accuracy, report['accuracy'], report\n",
    "\n",
    "# Perform cross-validation and evaluation for Naive Bayes classifier with unigram features\n",
    "print(\"Results for Naive Bayes Classifier with unigram features:\")\n",
    "nb_classifier_unigram = NaiveBayesClassifier.train(unigram_feature_sets)\n",
    "\n",
    "total_accuracy_unigram = 0\n",
    "total_report_unigram = None\n",
    "\n",
    "for train_index, test_index in kf.split(unigram_feature_sets):\n",
    "    train_set = [unigram_feature_sets[i] for i in train_index]\n",
    "    test_set = [unigram_feature_sets[i] for i in test_index]\n",
    "    accuracy, _, report = train_and_evaluate(nb_classifier_unigram, train_set, test_set)\n",
    "    total_accuracy_unigram += accuracy\n",
    "\n",
    "    if total_report_unigram is None:\n",
    "        total_report_unigram = report\n",
    "    else:\n",
    "        for label, metrics in report.items():\n",
    "            if label != 'accuracy':\n",
    "                total_report_unigram[label]['precision'] += metrics['precision']\n",
    "                total_report_unigram[label]['recall'] += metrics['recall']\n",
    "                total_report_unigram[label]['f1-score'] += metrics['f1-score']\n",
    "    \n",
    "num_iterations = kf.get_n_splits()\n",
    "average_accuracy_unigram = total_accuracy_unigram / num_iterations\n",
    "\n",
    "print(\"Average Accuracy (Unigram):\", average_accuracy_unigram)\n",
    "\n",
    "for label, metrics in total_report_unigram.items():\n",
    "    if label != 'accuracy':\n",
    "        metrics['precision'] /= num_iterations\n",
    "        metrics['recall'] /= num_iterations\n",
    "        metrics['f1-score'] /= num_iterations\n",
    "        print(f\"\\nAverage metrics for '{label}':\")\n",
    "        print(\"Precision:\", metrics['precision'])\n",
    "        print(\"Recall:\", metrics['recall'])\n",
    "        print(\"F1-score:\", metrics['f1-score'])\n",
    "\n",
    "# Perform cross-validation and evaluation for Naive Bayes classifier with bigram features\n",
    "print(\"\\nResults for Naive Bayes Classifier with bigram features:\")\n",
    "nb_classifier_bigram = NaiveBayesClassifier.train(bigram_feature_sets)\n",
    "\n",
    "total_accuracy_bigram = 0\n",
    "total_report_bigram = None\n",
    "\n",
    "for train_index, test_index in kf.split(bigram_feature_sets):\n",
    "    train_set = [bigram_feature_sets[i] for i in train_index]\n",
    "    test_set = [bigram_feature_sets[i] for i in test_index]\n",
    "    accuracy, _, report = train_and_evaluate(nb_classifier_bigram, train_set, test_set)\n",
    "    total_accuracy_bigram += accuracy\n",
    "\n",
    "    if total_report_bigram is None:\n",
    "        total_report_bigram = report\n",
    "    else:\n",
    "        for label, metrics in report.items():\n",
    "            if label != 'accuracy':\n",
    "                total_report_bigram[label]['precision'] += metrics['precision']\n",
    "                total_report_bigram[label]['recall'] += metrics['recall']\n",
    "                total_report_bigram[label]['f1-score'] += metrics['f1-score']\n",
    "    \n",
    "average_accuracy_bigram = total_accuracy_bigram / num_iterations\n",
    "\n",
    "print(\"Average Accuracy (Bigram):\", average_accuracy_bigram)\n",
    "\n",
    "for label, metrics in total_report_bigram.items():\n",
    "    if label != 'accuracy':\n",
    "        metrics['precision'] /= num_iterations\n",
    "        metrics['recall'] /= num_iterations\n",
    "        metrics['f1-score'] /= num_iterations\n",
    "        print(f\"\\nAverage metrics for '{label}':\")\n",
    "        print(\"Precision:\", metrics['precision'])\n",
    "        print(\"Recall:\", metrics['recall'])\n",
    "        print(\"F1-score:\", metrics['f1-score'])\n",
    "\n",
    "# Perform cross-validation and evaluation for Naive Bayes classifier with POS features\n",
    "print(\"\\nResults for Naive Bayes Classifier with POS features:\")\n",
    "nb_classifier_pos = NaiveBayesClassifier.train(pos_feature_sets)\n",
    "\n",
    "total_accuracy_pos = 0\n",
    "total_report_pos = None\n",
    "\n",
    "for train_index, test_index in kf.split(pos_feature_sets):\n",
    "    train_set = [pos_feature_sets[i] for i in train_index]\n",
    "    test_set = [pos_feature_sets[i] for i in test_index]\n",
    "    accuracy, _, report = train_and_evaluate(nb_classifier_pos, train_set, test_set)\n",
    "    total_accuracy_pos += accuracy\n",
    "\n",
    "    if total_report_pos is None:\n",
    "        total_report_pos = report\n",
    "    else:\n",
    "        for label, metrics in report.items():\n",
    "            if label != 'accuracy':\n",
    "                total_report_pos[label]['precision'] += metrics['precision']\n",
    "                total_report_pos[label]['recall'] += metrics['recall']\n",
    "                total_report_pos[label]['f1-score'] += metrics['f1-score']\n",
    "    \n",
    "average_accuracy_pos = total_accuracy_pos / num_iterations\n",
    "\n",
    "print(\"Average Accuracy (POS):\", average_accuracy_pos)\n",
    "\n",
    "for label, metrics in total_report_pos.items():\n",
    "    if label != 'accuracy':\n",
    "        metrics['precision'] /= num_iterations\n",
    "        metrics['recall'] /= num_iterations\n",
    "        metrics['f1-score'] /= num_iterations\n",
    "        print(f\"\\nAverage metrics for '{label}':\")\n",
    "        print(\"Precision:\", metrics['precision'])\n",
    "        print(\"Recall:\", metrics['recall'])\n",
    "        print(\"F1-score:\", metrics['f1-score'])\n",
    "\n",
    "# Perform cross-validation and evaluation for Naive Bayes classifier with all features\n",
    "print(\"\\nResults for Naive Bayes Classifier with all features:\")\n",
    "nb_classifier_all = NaiveBayesClassifier.train(all_feature_sets)\n",
    "\n",
    "total_accuracy_all = 0\n",
    "total_report_all = None\n",
    "\n",
    "for train_index, test_index in kf.split(all_feature_sets):\n",
    "    train_set = [all_feature_sets[i] for i in train_index]\n",
    "    test_set = [all_feature_sets[i] for i in test_index]\n",
    "    accuracy, _, report = train_and_evaluate(nb_classifier_all, train_set, test_set)\n",
    "    total_accuracy_all += accuracy\n",
    "\n",
    "    if total_report_all is None:\n",
    "        total_report_all = report\n",
    "    else:\n",
    "        for label, metrics in report.items():\n",
    "            if label != 'accuracy':\n",
    "                total_report_all[label]['precision'] += metrics['precision']\n",
    "                total_report_all[label]['recall'] += metrics['recall']\n",
    "                total_report_all[label]['f1-score'] += metrics['f1-score']\n",
    "    \n",
    "average_accuracy_all = total_accuracy_all / num_iterations\n",
    "\n",
    "print(\"Average Accuracy (All Features):\", average_accuracy_all)\n",
    "\n",
    "for label, metrics in total_report_all.items():\n",
    "    if label != 'accuracy':\n",
    "        metrics['precision'] /= num_iterations\n",
    "        metrics['recall'] /= num_iterations\n",
    "        metrics['f1-score'] /= num_iterations\n",
    "        print(f\"\\nAverage metrics for '{label}':\")\n",
    "        print(\"Precision:\", metrics['precision'])\n",
    "        print(\"Recall:\", metrics['recall'])\n",
    "        print(\"F1-score:\", metrics['f1-score'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Unigram Features with Stopword Filtering:\n",
      "Accuracy: 0.9385150812064965\n",
      "Precision: 0.9097772642196424\n",
      "Recall: 0.9561160235798499\n",
      "F1 Score: 0.9278812533248721\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       1.00      0.92      0.96      1244\n",
      "        spam       0.82      1.00      0.90       480\n",
      "\n",
      "    accuracy                           0.94      1724\n",
      "   macro avg       0.91      0.96      0.93      1724\n",
      "weighted avg       0.95      0.94      0.94      1724\n",
      "\n",
      "Accuracy: 0.9373549883990719\n",
      "Precision: 0.9148902258079473\n",
      "Recall: 0.9543916667195643\n",
      "F1 Score: 0.9297870896214675\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       1.00      0.91      0.95      1198\n",
      "        spam       0.83      1.00      0.91       526\n",
      "\n",
      "    accuracy                           0.94      1724\n",
      "   macro avg       0.91      0.95      0.93      1724\n",
      "weighted avg       0.95      0.94      0.94      1724\n",
      "\n",
      "Accuracy: 0.9373549883990719\n",
      "Precision: 0.9106880009534877\n",
      "Recall: 0.9536749942398209\n",
      "F1 Score: 0.9274862911266202\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       1.00      0.92      0.95      1230\n",
      "        spam       0.82      0.99      0.90       494\n",
      "\n",
      "    accuracy                           0.94      1724\n",
      "   macro avg       0.91      0.95      0.93      1724\n",
      "weighted avg       0.95      0.94      0.94      1724\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Experiment with Stop Words\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Step 1: Extract features with stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def extract_features_with_stopwords(emails):\n",
    "    features = []\n",
    "    for email_text in emails:\n",
    "        tokens = word_tokenize(email_text)\n",
    "        filtered_tokens = [token.lower() for token in tokens if token.isalpha() and token.lower() not in stop_words]\n",
    "        features.extend(filtered_tokens)\n",
    "    return features\n",
    "\n",
    "ham_features_with_stopwords = extract_features_with_stopwords(ham_emails)\n",
    "spam_features_with_stopwords = extract_features_with_stopwords(spam_emails)\n",
    "all_features_with_stopwords = FreqDist(ham_features_with_stopwords + spam_features_with_stopwords)\n",
    "word_features_with_stopwords = [feature for feature, _ in all_features_with_stopwords.most_common(N)]\n",
    "\n",
    "def email_features_with_stopwords(email_text):\n",
    "    email_words = set(word_tokenize(email_text))\n",
    "    features = {}\n",
    "    for word in word_features_with_stopwords:\n",
    "        features[word] = (word in email_words)\n",
    "    return features\n",
    "\n",
    "unigram_feature_sets_with_stopwords = [(email_features_with_stopwords(email), 'ham') for email in ham_emails] + [(email_features_with_stopwords(email), 'spam') for email in spam_emails]\n",
    "\n",
    "print(\"Results for Unigram Features with Stopword Filtering:\")\n",
    "kf = KFold(n_splits=3, shuffle=True)\n",
    "\n",
    "for train_index, test_index in kf.split(unigram_feature_sets_with_stopwords):\n",
    "    train_set = [unigram_feature_sets_with_stopwords[i] for i in train_index]\n",
    "    test_set = [unigram_feature_sets_with_stopwords[i] for i in test_index]\n",
    "    nb_classifier_with_stopwords = nltk.NaiveBayesClassifier.train(train_set)    \n",
    "    true_labels = [label for _, label in test_set]\n",
    "    predicted_labels = [nb_classifier_with_stopwords.classify(features) for features, _ in test_set]    \n",
    "    report = classification_report(true_labels, predicted_labels, output_dict=True)\n",
    "    accuracy = report['accuracy']\n",
    "    precision = report['macro avg']['precision']\n",
    "    recall = report['macro avg']['recall']\n",
    "    f1_score = report['macro avg']['f1-score']    \n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1 Score:\", f1_score)\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(true_labels, predicted_labels))\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Unigram Features with Negation Representation:\n",
      "Accuracy: 0.9425754060324826\n",
      "Precision: 0.9198262330926397\n",
      "Recall: 0.9562932115638165\n",
      "F1 Score: 0.9344493489524823\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       1.00      0.92      0.96      1210\n",
      "        spam       0.84      0.99      0.91       514\n",
      "\n",
      "    accuracy                           0.94      1724\n",
      "   macro avg       0.92      0.96      0.93      1724\n",
      "weighted avg       0.95      0.94      0.94      1724\n",
      "\n",
      "Accuracy: 0.9431554524361949\n",
      "Precision: 0.9184416206068675\n",
      "Recall: 0.9576246370291603\n",
      "F1 Score: 0.9342005234297108\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       1.00      0.92      0.96      1225\n",
      "        spam       0.84      0.99      0.91       499\n",
      "\n",
      "    accuracy                           0.94      1724\n",
      "   macro avg       0.92      0.96      0.93      1724\n",
      "weighted avg       0.95      0.94      0.94      1724\n",
      "\n",
      "Accuracy: 0.9501160092807425\n",
      "Precision: 0.9257080027793261\n",
      "Recall: 0.9627485188880165\n",
      "F1 Score: 0.9411788976557888\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       1.00      0.93      0.96      1237\n",
      "        spam       0.85      0.99      0.92       487\n",
      "\n",
      "    accuracy                           0.95      1724\n",
      "   macro avg       0.93      0.96      0.94      1724\n",
      "weighted avg       0.96      0.95      0.95      1724\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Experiment with Negation Representation\n",
    "negation_words = set([\"not\", \"no\", \"n't\"])\n",
    "\n",
    "def email_features_with_negation(email_text):\n",
    "    email_words = set(word_tokenize(email_text))\n",
    "    features = {}\n",
    "    for word in word_features_with_stopwords:\n",
    "        features[word] = (word in email_words)\n",
    "        features[\"not_\" + word] = (\"not\" in email_words and word in email_words)\n",
    "    return features\n",
    "\n",
    "unigram_feature_sets_with_negation = [(email_features_with_negation(email), 'ham') for email in ham_emails] + [(email_features_with_negation(email), 'spam') for email in spam_emails]\n",
    "\n",
    "print(\"Results for Unigram Features with Negation Representation:\")\n",
    "for train_index, test_index in kf.split(unigram_feature_sets_with_negation):\n",
    "    train_set = [unigram_feature_sets_with_negation[i] for i in train_index]\n",
    "    test_set = [unigram_feature_sets_with_negation[i] for i in test_index]\n",
    "\n",
    "    nb_classifier_with_negation = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "    true_labels = [label for _, label in test_set]\n",
    "    predicted_labels = [nb_classifier_with_negation.classify(features) for features, _ in test_set]\n",
    "\n",
    "    report = classification_report(true_labels, predicted_labels, output_dict=True)\n",
    "    accuracy = report['accuracy']\n",
    "    precision = report['macro avg']['precision']\n",
    "    recall = report['macro avg']['recall']\n",
    "    f1_score = report['macro avg']['f1-score']\n",
    "\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1 Score:\", f1_score)\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(true_labels, predicted_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
